{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fuck']\n",
      "['fuck', 'fuck']\n"
     ]
    }
   ],
   "source": [
    "def f(a=[]):\n",
    "  a.append(\"fuck\")\n",
    "  return a\n",
    "\n",
    "print(f())\n",
    "print(f())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def construct_prompt(sample):\n",
    "    question_prompt = \"Question:\\n\" \\\n",
    "                      \"{question}\\n\".format(question=sample[\"input\"])\n",
    "\n",
    "    # options = [\n",
    "    #     (chr(ord(\"A\") + i), opt)\n",
    "    #     for i, opt in enumerate(sample[\"options\"])\n",
    "    # ]\n",
    "    options = sample[\"options\"]\n",
    "    option_prompt = \"Options:\\n\" + \"\\n\".join([\"{k}) {v}\".format(k=chr(k + ord(\"A\")), v=v) for k, v in enumerate(options)]) + \"\\n\"\n",
    "\n",
    "    system_prompt = \"Imagine you are a doctor, and please suggest the treatmeat based on the given patient information above. You need to choose one option from the following list.\\n\"\n",
    "    instruction_prompt = \"Answer only with the option index such as A/B/C/D/E/F/G in plain text.\"\n",
    "    cot_instruction_prompt = \"Reason step-by-step before answering. Answer only with the option index such as A/B/C/D/E/F/G in plain text. Your final output should strictly follow this format:\\n\" \\\n",
    "                             \"<Reason>{your step-by-step reasoning}</Reason><Answer>{your answer}</Answer>\"\n",
    "\n",
    "    prompt = \"\\n\".join([question_prompt, system_prompt, option_prompt, instruction_prompt])\n",
    "    prompt_with_cot = \"\\n\".join(\n",
    "        [question_prompt, system_prompt, option_prompt, cot_instruction_prompt]\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"prompt\": prompt,\n",
    "        \"prompt_with_cot\": prompt_with_cot\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = {\n",
    "    \"input\": \"{question}\",\n",
    "    \"options\": {\"A\": \"A\", \"B\": \"B\"}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:\n",
      "{question}\n",
      "\n",
      "Imagine you are a doctor, and please suggest the treatmeat based on the given patient information above. You need to choose one option from the following list.\n",
      "\n",
      "Options:\n",
      "A) A\n",
      "B) B\n",
      "\n",
      "Answer only with the option index such as A/B/C/D/E/F/G in plain text.\n"
     ]
    }
   ],
   "source": [
    "print(construct_prompt(sample)[\"prompt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's recreate the data carefully to ensure all columns have the correct length\n",
    "\n",
    "data = {\n",
    "    \"Task\": [\n",
    "        \"Concept Recognition\", \"Concept Recognition\", \"Concept Recognition\", \"Concept Recognition\", \n",
    "        \"Concept Recognition\", \"Concept Recognition\", \"Concept Recognition\", \"Clinical Decision Support\", \n",
    "        \"Clinical Decision Support\", \"Clinical Decision Support\", \"Clinical Decision Support\", \n",
    "        \"Clinical Decision Support\", \"Reasoning\", \"Reasoning\", \"Reasoning\", \"Reasoning\", \n",
    "        \"Knowledge\", \"Knowledge\", \"Knowledge\", \"Knowledge\", \"Knowledge\", \"Mathematical\"\n",
    "    ],\n",
    "    \"Dataset\": [\n",
    "        \"PMC-Patient\", \"PICO-Participant\", \"PICO-Intervention\", \"PICO-Outcome\", \n",
    "        \"ADE Corpus\", \"BC5-disease\", \"BC5Chem\", \"DDXPlus\", \n",
    "        \"SEER\", \"MIMIC4ED -Hospitalization\", \"MIMIC4ED -72h ED Revisit\", \n",
    "        \"MIMIC4ED -Critical Triage\", \"MedNLI-Dis.\", \"PUBHEALTH Ver.\", \n",
    "        \"PubMedQA\", \"MedQA\", \"MedMCQA\", \"LancetQA\", \"NEJMQA\", \"Medbullets\", \"MedCalc-Bench\"\n",
    "    ],\n",
    "    \"o1\": [\n",
    "        76.4, 75.0, 77.5, 67.5, 75.0, 75.0, 71.2, 52.0, 72.4, 64.0, \n",
    "        59.7, 61.7, 88.0, 76.4, 75.0, 75.0, 95.0, 81.5, 91.2, 90.6, 34.9\n",
    "    ],\n",
    "    \"GPT-4\": [\n",
    "        75.7, 75.0, 75.0, 65.0, 85.0, 85.0, 71.2, 45.0, 38.0, 61.0, \n",
    "        58.0, 66.7, 84.0, 75.7, 52.8, 69.7, 95.0, 76.1, 83.5, 66.9, 25.5\n",
    "    ],\n",
    "    \"GPT-3.5\": [\n",
    "        74.4, 52.5, 75.0, 60.0, 90.0, 69.5, 43.1, 34.6, 5.0, 62.0, \n",
    "        53.6, 58.7, 57.0, 74.4, 25.4, 53.8, 88.5, 61.0, 65.0, 50.7, 10.8\n",
    "    ],\n",
    "    \"MEDITRON* (70B)\": [\n",
    "        72.2, 72.1, 46.6, 51.2, 95.7, 69.6, 4.3, 29.6, 68.3, 56.3, \n",
    "        48.5, 45.7, 60.9, 32.7, 74.4, 47.9, 58.8, 0.0, None, 0.0, None\n",
    "    ],\n",
    "    \"Llama3* (8B)\": [\n",
    "        96.0, 58.2, 79.1, 58.2, 69.6, 25.3, 19.5, 33.8, 56.1, 39.1, \n",
    "        9.3, 8.8, 63.9, 63.9, 73.0, 50.7, 50.7, None, None, None, None\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Concept Recognition', 'PMC-Patient'),\n",
       " ('Concept Recognition', 'PICO-Participant'),\n",
       " ('Concept Recognition', 'PICO-Intervention'),\n",
       " ('Concept Recognition', 'PICO-Outcome'),\n",
       " ('Concept Recognition', 'ADE Corpus'),\n",
       " ('Concept Recognition', 'BC5-disease'),\n",
       " ('Concept Recognition', 'BC5Chem'),\n",
       " ('Clinical Decision Support', 'DDXPlus'),\n",
       " ('Clinical Decision Support', 'SEER'),\n",
       " ('Clinical Decision Support', 'MIMIC4ED -Hospitalization'),\n",
       " ('Clinical Decision Support', 'MIMIC4ED -72h ED Revisit'),\n",
       " ('Clinical Decision Support', 'MIMIC4ED -Critical Triage'),\n",
       " ('Reasoning', 'MedNLI-Dis.'),\n",
       " ('Reasoning', 'PUBHEALTH Ver.'),\n",
       " ('Reasoning', 'PubMedQA'),\n",
       " ('Reasoning', 'MedQA'),\n",
       " ('Knowledge', 'MedMCQA'),\n",
       " ('Knowledge', 'LancetQA'),\n",
       " ('Knowledge', 'NEJMQA'),\n",
       " ('Knowledge', 'Medbullets'),\n",
       " ('Knowledge', 'MedCalc-Bench')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(t, d) for t, d in zip(data[\"Task\"], data[\"Dataset\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Task': 22,\n",
       " 'Dataset': 21,\n",
       " 'o1': 21,\n",
       " 'GPT-4': 21,\n",
       " 'GPT-3.5': 21,\n",
       " 'MEDITRON* (70B)': 21,\n",
       " 'Llama3* (8B)': 21}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_debug = {key: len(value) for key, value in data.items()}\n",
    "data_debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Calculate the mean score for each model in each aspect and task\u001b[39;00m\n\u001b[1;32m      4\u001b[0m mean_scores \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTask\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mmean()\n",
      "File \u001b[0;32m/opt/conda/envs/eval/lib/python3.10/site-packages/pandas/core/frame.py:778\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    772\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[1;32m    773\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[1;32m    774\u001b[0m     )\n\u001b[1;32m    776\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    777\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[0;32m--> 778\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    779\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[1;32m    780\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "File \u001b[0;32m/opt/conda/envs/eval/lib/python3.10/site-packages/pandas/core/internals/construction.py:503\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    499\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    500\u001b[0m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[1;32m    501\u001b[0m         arrays \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[0;32m--> 503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/eval/lib/python3.10/site-packages/pandas/core/internals/construction.py:114\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verify_integrity:\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;66;03m# figure out the index, if necessary\u001b[39;00m\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 114\u001b[0m         index \u001b[38;5;241m=\u001b[39m \u001b[43m_extract_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    116\u001b[0m         index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n",
      "File \u001b[0;32m/opt/conda/envs/eval/lib/python3.10/site-packages/pandas/core/internals/construction.py:677\u001b[0m, in \u001b[0;36m_extract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    675\u001b[0m lengths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(raw_lengths))\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(lengths) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 677\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll arrays must be of the same length\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    679\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m have_dicts:\n\u001b[1;32m    680\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    681\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMixing dicts with non-Series may lead to ambiguous ordering.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    682\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: All arrays must be of the same length"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Calculate the mean score for each model in each aspect and task\n",
    "mean_scores = df.groupby(\"Task\").mean()\n",
    "\n",
    "import ace_tools as tools; tools.display_dataframe_to_user(name=\"Mean Scores by Task\", dataframe=mean_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
